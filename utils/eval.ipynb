{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7643313-b095-4401-8e97-bbd3a476bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries we need\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatMlflow\n",
    "from langchain_openai import ChatOpenAI\n",
    "from statistics import median\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51247a-bedf-4f16-9bff-84957bcfc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure to use to get the output from the evaluation chain\n",
    "class EvalResultSchema(BaseModel):\n",
    "\n",
    "    match_score:int = Field(description=\"A scalar value representing how well the answer matches the ground truth\")\n",
    "    eval_description:str = Field(description=\"An explanation that justifies the score that was assigned for how well the answer matches ground truth\")\n",
    "\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=EvalResultSchema)\n",
    "format_instructions = pydantic_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9b9c6-0703-4320-8422-96e3f48aa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and setup the prompt to use for the evaluation\n",
    "\n",
    "# Uncomment  to use the AI gateway, change the name of the endpoint as required\n",
    "# eval_model = ChatMlflow(\n",
    "#     target_uri=os.environ[\"DOMINO_MLFLOW_DEPLOYMENTS\"],\n",
    "#     endpoint=\"chat-gpt35turbo-sm\",\n",
    "# )\n",
    "\n",
    "eval_model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "EVAL_PROMPT = \"\"\"\n",
    "Your goal is give an integer score in the range of 1 to 10 by matching how well the LLM Answer. \n",
    "While assigning a score just focus on the facts and entities that match with the Ground Truth Answer. Also give higher scores to answers that are terse.\n",
    "A bad answer will have a low integer score and a good match will have a high integer score\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "LLM Answer:\n",
    "{llm_answer}\n",
    "\n",
    "Ground Truth Answer:\n",
    "{gt_answer}\n",
    "\"\"\"\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_template(\n",
    "    template=EVAL_PROMPT\n",
    ")\n",
    "\n",
    "eval_chain = LLMChain(llm=eval_model, prompt=eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65ad9abf-7fe2-42a4-9d01-8c0a44b1a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the scores of how well the RAG response matches with the answer provided as ground truth\n",
    "def plot_match_scores_distributions(graph_rag_scores, rag_scores):\n",
    "    # Set the figure size and adjust the padding between and around the subplots\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the first set of match scores\n",
    "    plt.hist(graph_rag_scores, bins=10, alpha=0.5, color='#aec6cf', label='Graph RAG')\n",
    "    \n",
    "    # Plot the second set of match scores\n",
    "    plt.hist(rag_scores, bins=10, alpha=0.5, color='#ff6961', label='RAG')\n",
    "    \n",
    "    # Adding titles and labels\n",
    "    plt.title('Distribution of Match Scores')\n",
    "    plt.xlabel('Match Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Display legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute and print the median for both sets of match scores\n",
    "    median_graph_rag_score = median(graph_rag_scores)\n",
    "    median_rag_score = median(rag_scores)\n",
    "    print(f\"The median match score for Graph RAG is: {median_graph_rag_score}\")\n",
    "    print(f\"The median match score for RAG is: {median_rag_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b26b4d-b820-4a97-b272-c8b16b1c783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the scores from the response from the evaluation chain\n",
    "def extract_match_scores(results):\n",
    "    match_scores = [eval(result['text'])['match_score'] for result in results]\n",
    "    return(match_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f67a83c-d515-492a-bcaa-2c5eb3f42458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that loads the QA dataset for evaluation and gets the scores from the evaluation chain\n",
    "def get_match_scores(csv_file_name):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "    \n",
    "    # Step 2: Extract column values\n",
    "    llm_answers = df['llm_answer'].tolist()\n",
    "    gt_answers = df['gt_answer'].tolist()\n",
    "    \n",
    "    # Step 3: Format the data\n",
    "    batch_data = [{\"llm_answer\": llm_answer, \"gt_answer\": gt_answer, \"format_instructions\":format_instructions} for llm_answer, gt_answer in zip(llm_answers, gt_answers)]\n",
    "    \n",
    "    # Step 4: Call chain.batch\n",
    "    results = eval_chain.batch(batch_data)   \n",
    "    \n",
    "    match_scores = extract_match_scores(results)\n",
    "    return match_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da902f-853f-4072-b8e3-1a29d44ed099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
